{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4000500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afc7239",
   "metadata": {},
   "source": [
    "### Úprava surových dat: anonymizace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b26725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/input/respondents.csv\", delimiter='|')\n",
    "df = df.drop(columns=['first_name', 'last_name'])\n",
    "df.to_csv('data/respondents.csv', sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf5b60",
   "metadata": {},
   "source": [
    "### Načtení anonymizovaných dat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efda905",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/respondents.csv\", delimiter='|', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['descriptions'].str.contains('-') & (~df['descriptions'].str.contains('ex-'))].descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed244ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df[df['descriptions'].str.contains('A-E')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['descriptions'].str.contains('ID') & (~df.descriptions.str.contains('OM_ID'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b5a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['descriptions'].str.contains('kand_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['descriptions'].str.contains(\"'\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e99df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "zkratky = {'zast_': 'zastupitel ', 'kand_': 'kandidát ', 'posl': 'poslanec '}\n",
    "interpunkcni_z = '\",.;:_!?(){}-'\n",
    "stopword_file = 'data/stopwords-cs.json'\n",
    "political_parties = ['ods', 'kdu-čsl', 'čssd', 'ano', 'piráti', 'stan', 'ksčm', 'spd', 'top 09', 'ano 2011']\n",
    "\n",
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def replace_party(word: str) -> str:\n",
    "    if word in political_parties:\n",
    "        return \"strana\"\n",
    "    return word\n",
    "\n",
    "def unify_parties(column: pd.Series) -> pd.Series:\n",
    "    return column.apply(lambda row: [replace_party(x) for x in row])\n",
    "\n",
    "def remove_shortened_words(column: pd.Series, translations: dict = zkratky) -> pd.Series:\n",
    "    without_shortened = column\n",
    "    for shorter, full in translations.items():\n",
    "        without_shortened = without_shortened.str.replace(shorter, full)\n",
    "    return without_shortened\n",
    "\n",
    "def remove_interpunctions(column: pd.Series, interpunkcni_z: str = interpunkcni_z, sep: str =' ') -> pd.Series:\n",
    "    without_interpunction = column\n",
    "    for letter in interpunkcni_z:\n",
    "        without_interpunction = without_interpunction.str.replace(letter, sep)\n",
    "    return without_interpunction\n",
    "\n",
    "def split_into_words(column: pd.Series, sep=' ') -> pd.Series:\n",
    "    return column.str.split(sep)\n",
    "\n",
    "def remove_stop_words(column: pd.Series, filename: str = stopword_file) -> pd.Series:\n",
    "    with open(filename, 'r') as fd:\n",
    "        stop_words = json.load(fd)\n",
    "        return column.apply(lambda words: [word for word in words if word not in stop_words])\n",
    "def remove_empty(column: pd.Series) -> pd.Series:\n",
    "    return column.apply(lambda words: [word for word in words if word != ''])\n",
    "def remove_numbers(column: pd.Series) -> pd.Series:\n",
    "    return column.apply(lambda words: [word for word in words if not has_numbers(word)])\n",
    "\n",
    "def lemmatizate_word(word: str) -> str:\n",
    "    word = bytes(word, encoding='utf-8')\n",
    "    p = subprocess.Popen(\"lib/majka -f data/majka.w-lt| head -n1 | cut -d ':' -f1\",\n",
    "                         shell=True, stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.PIPE,\n",
    "                         stdin=subprocess.PIPE)\n",
    "    lemma, _ = p.communicate(input=word)\n",
    "    lemma = lemma.strip(b'\\n')\n",
    "    if len(lemma) == 0:\n",
    "        lemma = word\n",
    "    return lemma.decode(encoding='utf-8')\n",
    "\n",
    "def lemmatizate_words(words: list) -> list:\n",
    "    lemmatized = []\n",
    "    for word in words:\n",
    "        lemmatized.append(lemmatizate_word(word))\n",
    "    return lemmatized\n",
    "\n",
    "def lemmatizate(column: pd.Series) -> pd.Series:    \n",
    "    return column.apply(lemmatizate_words)\n",
    "\n",
    "def apply_pipeline(column: pd.Series, pipeline: list) -> pd.Series:\n",
    "    processed = column.copy()\n",
    "    for func in pipeline:\n",
    "        processed = func(processed)\n",
    "    return processed\n",
    "\n",
    "def lower(column: pd.Series) -> pd.Series:\n",
    "    return column.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pandarallel import pandarallel\n",
    "from lib.czech_stemmer.czech_stemmer import cz_stem\n",
    "pipeline = [lower, remove_shortened_words, remove_interpunctions, split_into_words, remove_empty, remove_numbers, remove_stop_words]\n",
    "\n",
    "df['descr_procc'] = apply_pipeline(df.descriptions, pipeline)\n",
    "\n",
    "pandarallel.initialize()\n",
    "df['descr_procc'] = df['descr_procc'].parallel_apply(lemmatizate_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef67d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['descr_stemmed_l'] = df['descr_procc'].apply(lambda x: [cz_stem(word) for word in x]) \n",
    "df['descr_stemmed_a'] = df['descr_procc'].apply(lambda x: [cz_stem(word, aggressive=True) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bde6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/proccessed.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257daf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "dda12f20db690ad826731bbb01ef5a6a40ea9bbf041b730d732ba5a193c3ab4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
